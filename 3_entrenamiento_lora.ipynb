{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uIBC4MXHaHEV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\DAVID SM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\DAVID SM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import json\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "t7CAdnwyaHEa"
      },
      "outputs": [],
      "source": [
        "from capas_gpt import TransformerBlock, LayerNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9Ssl0mpYaHEb"
      },
      "outputs": [],
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        tok_embeds = self.tok_emb(input_ids)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=input_ids.device))\n",
        "        x = tok_embeds + pos_embeds\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "\n",
        "        return {\"loss\": loss, \"logits\": logits}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "t0a1k4Adkvwy"
      },
      "outputs": [],
      "source": [
        "with open(\"config_gpt.json\", \"r\") as f:\n",
        "    cfg = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JU31qU6DaHEe",
        "outputId": "721f2446-e3c7-4fec-c485-e42082a8179d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 1280)\n",
              "  (pos_emb): Embedding(1024, 1280)\n",
              "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (12): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (13): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (14): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (15): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (16): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (17): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (18): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (19): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (20): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (21): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (22): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (23): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (24): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (25): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (26): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (27): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (28): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (29): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (30): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (31): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (32): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (33): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (34): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (35): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_path = \"modelo_gpt_custom.pth\"\n",
        "\n",
        "model = GPTModel(cfg)\n",
        "model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "i4baIDiKI0tc"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def generate_text(model,tokenizer,prompt,seed=42,max_new_tokens=50,temperature=0.9,top_k=50,top_p=0.95,repetition_penalty=1.1):\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "        random.seed(seed)\n",
        "\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    generated_ids = input_ids.clone()\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        input_ids_cropped = generated_ids[:, -cfg[\"context_length\"]:]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids_cropped)\n",
        "            logits = outputs[\"logits\"][:, -1, :]\n",
        "\n",
        "        for token_id in set(generated_ids[0].tolist()):\n",
        "            logits[0, token_id] /= repetition_penalty\n",
        "\n",
        "        logits = logits / temperature\n",
        "\n",
        "        if top_k > 0:\n",
        "            values, _ = torch.topk(logits, top_k)\n",
        "            threshold = values[:, -1].unsqueeze(-1)\n",
        "            logits[logits < threshold] = -float(\"Inf\")\n",
        "\n",
        "        if top_p < 1.0:\n",
        "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "            probs = F.softmax(sorted_logits, dim=-1)\n",
        "            cumulative_probs = torch.cumsum(probs, dim=-1)\n",
        "\n",
        "            sorted_indices_to_remove = cumulative_probs > top_p\n",
        "            sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
        "            sorted_indices_to_remove[:, 0] = False\n",
        "\n",
        "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "            logits[0, indices_to_remove] = -float(\"Inf\")\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "        generated_ids = torch.cat((generated_ids, next_token), dim=1)\n",
        "\n",
        "        if next_token.item() == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return output_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBBj5QXaO1RA",
        "outputId": "c3f1df34-dba4-4d05-ec81-6b6041519f6e"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEZzrlppN24l",
        "outputId": "bee6da46-3bc2-4cc0-ac4e-aa170b362677"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I was walking in the park when I saw someone walk past me who said, 'Hey man you look like a big dude.'\n",
            "\n",
            "\"Well my son says to him, 'Dude you don't know what's going on with your mother.' And he goes and tells his friend where\n"
          ]
        }
      ],
      "source": [
        "prompt = \"I was walking in the park when I saw\"\n",
        "output = generate_text(model,tokenizer,prompt,repetition_penalty=1.2)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sYX9R_cYN2Kz"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ndy1MICpaHEf"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class LoRALayer(torch.nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
        "        super().__init__()\n",
        "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
        "        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
        "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.alpha * (x @ self.A @ self.B)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5g2OvZ_VaHEh"
      },
      "outputs": [],
      "source": [
        "class LinearWithLoRA(torch.nn.Module):\n",
        "    def __init__(self, linear, rank, alpha):\n",
        "        super().__init__()\n",
        "        self.linear = linear\n",
        "        self.lora = LoRALayer(\n",
        "            linear.in_features, linear.out_features, rank, alpha\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x) + self.lora(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7N_IXeQ7eBM",
        "outputId": "8c3d3b2d-f0f1-4e05-ba59-0026adcd1b72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'torch.nn.modules.linear.Linear'>\n"
          ]
        }
      ],
      "source": [
        "print(type(model.out_head))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dYOD_-67owU",
        "outputId": "a6b9b321-9fa0-45f2-e35f-3bca14e03851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ": GPTModel\n",
            "tok_emb: Embedding\n",
            "pos_emb: Embedding\n",
            "drop_emb: Dropout\n",
            "trf_blocks: Sequential\n",
            "trf_blocks.0: TransformerBlock\n",
            "trf_blocks.0.att: MultiHeadAttention\n",
            "trf_blocks.0.att.W_query: Linear\n",
            "trf_blocks.0.att.W_key: Linear\n",
            "trf_blocks.0.att.W_value: Linear\n",
            "trf_blocks.0.att.out_proj: Linear\n",
            "trf_blocks.0.att.dropout: Dropout\n",
            "trf_blocks.0.ff: FeedForward\n",
            "trf_blocks.0.ff.layers: Sequential\n",
            "trf_blocks.0.ff.layers.0: Linear\n",
            "trf_blocks.0.ff.layers.1: GELU\n",
            "trf_blocks.0.ff.layers.2: Linear\n",
            "trf_blocks.0.norm1: LayerNorm\n",
            "trf_blocks.0.norm2: LayerNorm\n",
            "trf_blocks.0.drop_shortcut: Dropout\n",
            "trf_blocks.1: TransformerBlock\n",
            "trf_blocks.1.att: MultiHeadAttention\n",
            "trf_blocks.1.att.W_query: Linear\n",
            "trf_blocks.1.att.W_key: Linear\n",
            "trf_blocks.1.att.W_value: Linear\n",
            "trf_blocks.1.att.out_proj: Linear\n",
            "trf_blocks.1.att.dropout: Dropout\n",
            "trf_blocks.1.ff: FeedForward\n",
            "trf_blocks.1.ff.layers: Sequential\n",
            "trf_blocks.1.ff.layers.0: Linear\n",
            "trf_blocks.1.ff.layers.1: GELU\n",
            "trf_blocks.1.ff.layers.2: Linear\n",
            "trf_blocks.1.norm1: LayerNorm\n",
            "trf_blocks.1.norm2: LayerNorm\n",
            "trf_blocks.1.drop_shortcut: Dropout\n",
            "trf_blocks.2: TransformerBlock\n",
            "trf_blocks.2.att: MultiHeadAttention\n",
            "trf_blocks.2.att.W_query: Linear\n",
            "trf_blocks.2.att.W_key: Linear\n",
            "trf_blocks.2.att.W_value: Linear\n",
            "trf_blocks.2.att.out_proj: Linear\n",
            "trf_blocks.2.att.dropout: Dropout\n",
            "trf_blocks.2.ff: FeedForward\n",
            "trf_blocks.2.ff.layers: Sequential\n",
            "trf_blocks.2.ff.layers.0: Linear\n",
            "trf_blocks.2.ff.layers.1: GELU\n",
            "trf_blocks.2.ff.layers.2: Linear\n",
            "trf_blocks.2.norm1: LayerNorm\n",
            "trf_blocks.2.norm2: LayerNorm\n",
            "trf_blocks.2.drop_shortcut: Dropout\n",
            "trf_blocks.3: TransformerBlock\n",
            "trf_blocks.3.att: MultiHeadAttention\n",
            "trf_blocks.3.att.W_query: Linear\n",
            "trf_blocks.3.att.W_key: Linear\n",
            "trf_blocks.3.att.W_value: Linear\n",
            "trf_blocks.3.att.out_proj: Linear\n",
            "trf_blocks.3.att.dropout: Dropout\n",
            "trf_blocks.3.ff: FeedForward\n",
            "trf_blocks.3.ff.layers: Sequential\n",
            "trf_blocks.3.ff.layers.0: Linear\n",
            "trf_blocks.3.ff.layers.1: GELU\n",
            "trf_blocks.3.ff.layers.2: Linear\n",
            "trf_blocks.3.norm1: LayerNorm\n",
            "trf_blocks.3.norm2: LayerNorm\n",
            "trf_blocks.3.drop_shortcut: Dropout\n",
            "trf_blocks.4: TransformerBlock\n",
            "trf_blocks.4.att: MultiHeadAttention\n",
            "trf_blocks.4.att.W_query: Linear\n",
            "trf_blocks.4.att.W_key: Linear\n",
            "trf_blocks.4.att.W_value: Linear\n",
            "trf_blocks.4.att.out_proj: Linear\n",
            "trf_blocks.4.att.dropout: Dropout\n",
            "trf_blocks.4.ff: FeedForward\n",
            "trf_blocks.4.ff.layers: Sequential\n",
            "trf_blocks.4.ff.layers.0: Linear\n",
            "trf_blocks.4.ff.layers.1: GELU\n",
            "trf_blocks.4.ff.layers.2: Linear\n",
            "trf_blocks.4.norm1: LayerNorm\n",
            "trf_blocks.4.norm2: LayerNorm\n",
            "trf_blocks.4.drop_shortcut: Dropout\n",
            "trf_blocks.5: TransformerBlock\n",
            "trf_blocks.5.att: MultiHeadAttention\n",
            "trf_blocks.5.att.W_query: Linear\n",
            "trf_blocks.5.att.W_key: Linear\n",
            "trf_blocks.5.att.W_value: Linear\n",
            "trf_blocks.5.att.out_proj: Linear\n",
            "trf_blocks.5.att.dropout: Dropout\n",
            "trf_blocks.5.ff: FeedForward\n",
            "trf_blocks.5.ff.layers: Sequential\n",
            "trf_blocks.5.ff.layers.0: Linear\n",
            "trf_blocks.5.ff.layers.1: GELU\n",
            "trf_blocks.5.ff.layers.2: Linear\n",
            "trf_blocks.5.norm1: LayerNorm\n",
            "trf_blocks.5.norm2: LayerNorm\n",
            "trf_blocks.5.drop_shortcut: Dropout\n",
            "trf_blocks.6: TransformerBlock\n",
            "trf_blocks.6.att: MultiHeadAttention\n",
            "trf_blocks.6.att.W_query: Linear\n",
            "trf_blocks.6.att.W_key: Linear\n",
            "trf_blocks.6.att.W_value: Linear\n",
            "trf_blocks.6.att.out_proj: Linear\n",
            "trf_blocks.6.att.dropout: Dropout\n",
            "trf_blocks.6.ff: FeedForward\n",
            "trf_blocks.6.ff.layers: Sequential\n",
            "trf_blocks.6.ff.layers.0: Linear\n",
            "trf_blocks.6.ff.layers.1: GELU\n",
            "trf_blocks.6.ff.layers.2: Linear\n",
            "trf_blocks.6.norm1: LayerNorm\n",
            "trf_blocks.6.norm2: LayerNorm\n",
            "trf_blocks.6.drop_shortcut: Dropout\n",
            "trf_blocks.7: TransformerBlock\n",
            "trf_blocks.7.att: MultiHeadAttention\n",
            "trf_blocks.7.att.W_query: Linear\n",
            "trf_blocks.7.att.W_key: Linear\n",
            "trf_blocks.7.att.W_value: Linear\n",
            "trf_blocks.7.att.out_proj: Linear\n",
            "trf_blocks.7.att.dropout: Dropout\n",
            "trf_blocks.7.ff: FeedForward\n",
            "trf_blocks.7.ff.layers: Sequential\n",
            "trf_blocks.7.ff.layers.0: Linear\n",
            "trf_blocks.7.ff.layers.1: GELU\n",
            "trf_blocks.7.ff.layers.2: Linear\n",
            "trf_blocks.7.norm1: LayerNorm\n",
            "trf_blocks.7.norm2: LayerNorm\n",
            "trf_blocks.7.drop_shortcut: Dropout\n",
            "trf_blocks.8: TransformerBlock\n",
            "trf_blocks.8.att: MultiHeadAttention\n",
            "trf_blocks.8.att.W_query: Linear\n",
            "trf_blocks.8.att.W_key: Linear\n",
            "trf_blocks.8.att.W_value: Linear\n",
            "trf_blocks.8.att.out_proj: Linear\n",
            "trf_blocks.8.att.dropout: Dropout\n",
            "trf_blocks.8.ff: FeedForward\n",
            "trf_blocks.8.ff.layers: Sequential\n",
            "trf_blocks.8.ff.layers.0: Linear\n",
            "trf_blocks.8.ff.layers.1: GELU\n",
            "trf_blocks.8.ff.layers.2: Linear\n",
            "trf_blocks.8.norm1: LayerNorm\n",
            "trf_blocks.8.norm2: LayerNorm\n",
            "trf_blocks.8.drop_shortcut: Dropout\n",
            "trf_blocks.9: TransformerBlock\n",
            "trf_blocks.9.att: MultiHeadAttention\n",
            "trf_blocks.9.att.W_query: Linear\n",
            "trf_blocks.9.att.W_key: Linear\n",
            "trf_blocks.9.att.W_value: Linear\n",
            "trf_blocks.9.att.out_proj: Linear\n",
            "trf_blocks.9.att.dropout: Dropout\n",
            "trf_blocks.9.ff: FeedForward\n",
            "trf_blocks.9.ff.layers: Sequential\n",
            "trf_blocks.9.ff.layers.0: Linear\n",
            "trf_blocks.9.ff.layers.1: GELU\n",
            "trf_blocks.9.ff.layers.2: Linear\n",
            "trf_blocks.9.norm1: LayerNorm\n",
            "trf_blocks.9.norm2: LayerNorm\n",
            "trf_blocks.9.drop_shortcut: Dropout\n",
            "trf_blocks.10: TransformerBlock\n",
            "trf_blocks.10.att: MultiHeadAttention\n",
            "trf_blocks.10.att.W_query: Linear\n",
            "trf_blocks.10.att.W_key: Linear\n",
            "trf_blocks.10.att.W_value: Linear\n",
            "trf_blocks.10.att.out_proj: Linear\n",
            "trf_blocks.10.att.dropout: Dropout\n",
            "trf_blocks.10.ff: FeedForward\n",
            "trf_blocks.10.ff.layers: Sequential\n",
            "trf_blocks.10.ff.layers.0: Linear\n",
            "trf_blocks.10.ff.layers.1: GELU\n",
            "trf_blocks.10.ff.layers.2: Linear\n",
            "trf_blocks.10.norm1: LayerNorm\n",
            "trf_blocks.10.norm2: LayerNorm\n",
            "trf_blocks.10.drop_shortcut: Dropout\n",
            "trf_blocks.11: TransformerBlock\n",
            "trf_blocks.11.att: MultiHeadAttention\n",
            "trf_blocks.11.att.W_query: Linear\n",
            "trf_blocks.11.att.W_key: Linear\n",
            "trf_blocks.11.att.W_value: Linear\n",
            "trf_blocks.11.att.out_proj: Linear\n",
            "trf_blocks.11.att.dropout: Dropout\n",
            "trf_blocks.11.ff: FeedForward\n",
            "trf_blocks.11.ff.layers: Sequential\n",
            "trf_blocks.11.ff.layers.0: Linear\n",
            "trf_blocks.11.ff.layers.1: GELU\n",
            "trf_blocks.11.ff.layers.2: Linear\n",
            "trf_blocks.11.norm1: LayerNorm\n",
            "trf_blocks.11.norm2: LayerNorm\n",
            "trf_blocks.11.drop_shortcut: Dropout\n",
            "trf_blocks.12: TransformerBlock\n",
            "trf_blocks.12.att: MultiHeadAttention\n",
            "trf_blocks.12.att.W_query: Linear\n",
            "trf_blocks.12.att.W_key: Linear\n",
            "trf_blocks.12.att.W_value: Linear\n",
            "trf_blocks.12.att.out_proj: Linear\n",
            "trf_blocks.12.att.dropout: Dropout\n",
            "trf_blocks.12.ff: FeedForward\n",
            "trf_blocks.12.ff.layers: Sequential\n",
            "trf_blocks.12.ff.layers.0: Linear\n",
            "trf_blocks.12.ff.layers.1: GELU\n",
            "trf_blocks.12.ff.layers.2: Linear\n",
            "trf_blocks.12.norm1: LayerNorm\n",
            "trf_blocks.12.norm2: LayerNorm\n",
            "trf_blocks.12.drop_shortcut: Dropout\n",
            "trf_blocks.13: TransformerBlock\n",
            "trf_blocks.13.att: MultiHeadAttention\n",
            "trf_blocks.13.att.W_query: Linear\n",
            "trf_blocks.13.att.W_key: Linear\n",
            "trf_blocks.13.att.W_value: Linear\n",
            "trf_blocks.13.att.out_proj: Linear\n",
            "trf_blocks.13.att.dropout: Dropout\n",
            "trf_blocks.13.ff: FeedForward\n",
            "trf_blocks.13.ff.layers: Sequential\n",
            "trf_blocks.13.ff.layers.0: Linear\n",
            "trf_blocks.13.ff.layers.1: GELU\n",
            "trf_blocks.13.ff.layers.2: Linear\n",
            "trf_blocks.13.norm1: LayerNorm\n",
            "trf_blocks.13.norm2: LayerNorm\n",
            "trf_blocks.13.drop_shortcut: Dropout\n",
            "trf_blocks.14: TransformerBlock\n",
            "trf_blocks.14.att: MultiHeadAttention\n",
            "trf_blocks.14.att.W_query: Linear\n",
            "trf_blocks.14.att.W_key: Linear\n",
            "trf_blocks.14.att.W_value: Linear\n",
            "trf_blocks.14.att.out_proj: Linear\n",
            "trf_blocks.14.att.dropout: Dropout\n",
            "trf_blocks.14.ff: FeedForward\n",
            "trf_blocks.14.ff.layers: Sequential\n",
            "trf_blocks.14.ff.layers.0: Linear\n",
            "trf_blocks.14.ff.layers.1: GELU\n",
            "trf_blocks.14.ff.layers.2: Linear\n",
            "trf_blocks.14.norm1: LayerNorm\n",
            "trf_blocks.14.norm2: LayerNorm\n",
            "trf_blocks.14.drop_shortcut: Dropout\n",
            "trf_blocks.15: TransformerBlock\n",
            "trf_blocks.15.att: MultiHeadAttention\n",
            "trf_blocks.15.att.W_query: Linear\n",
            "trf_blocks.15.att.W_key: Linear\n",
            "trf_blocks.15.att.W_value: Linear\n",
            "trf_blocks.15.att.out_proj: Linear\n",
            "trf_blocks.15.att.dropout: Dropout\n",
            "trf_blocks.15.ff: FeedForward\n",
            "trf_blocks.15.ff.layers: Sequential\n",
            "trf_blocks.15.ff.layers.0: Linear\n",
            "trf_blocks.15.ff.layers.1: GELU\n",
            "trf_blocks.15.ff.layers.2: Linear\n",
            "trf_blocks.15.norm1: LayerNorm\n",
            "trf_blocks.15.norm2: LayerNorm\n",
            "trf_blocks.15.drop_shortcut: Dropout\n",
            "trf_blocks.16: TransformerBlock\n",
            "trf_blocks.16.att: MultiHeadAttention\n",
            "trf_blocks.16.att.W_query: Linear\n",
            "trf_blocks.16.att.W_key: Linear\n",
            "trf_blocks.16.att.W_value: Linear\n",
            "trf_blocks.16.att.out_proj: Linear\n",
            "trf_blocks.16.att.dropout: Dropout\n",
            "trf_blocks.16.ff: FeedForward\n",
            "trf_blocks.16.ff.layers: Sequential\n",
            "trf_blocks.16.ff.layers.0: Linear\n",
            "trf_blocks.16.ff.layers.1: GELU\n",
            "trf_blocks.16.ff.layers.2: Linear\n",
            "trf_blocks.16.norm1: LayerNorm\n",
            "trf_blocks.16.norm2: LayerNorm\n",
            "trf_blocks.16.drop_shortcut: Dropout\n",
            "trf_blocks.17: TransformerBlock\n",
            "trf_blocks.17.att: MultiHeadAttention\n",
            "trf_blocks.17.att.W_query: Linear\n",
            "trf_blocks.17.att.W_key: Linear\n",
            "trf_blocks.17.att.W_value: Linear\n",
            "trf_blocks.17.att.out_proj: Linear\n",
            "trf_blocks.17.att.dropout: Dropout\n",
            "trf_blocks.17.ff: FeedForward\n",
            "trf_blocks.17.ff.layers: Sequential\n",
            "trf_blocks.17.ff.layers.0: Linear\n",
            "trf_blocks.17.ff.layers.1: GELU\n",
            "trf_blocks.17.ff.layers.2: Linear\n",
            "trf_blocks.17.norm1: LayerNorm\n",
            "trf_blocks.17.norm2: LayerNorm\n",
            "trf_blocks.17.drop_shortcut: Dropout\n",
            "trf_blocks.18: TransformerBlock\n",
            "trf_blocks.18.att: MultiHeadAttention\n",
            "trf_blocks.18.att.W_query: Linear\n",
            "trf_blocks.18.att.W_key: Linear\n",
            "trf_blocks.18.att.W_value: Linear\n",
            "trf_blocks.18.att.out_proj: Linear\n",
            "trf_blocks.18.att.dropout: Dropout\n",
            "trf_blocks.18.ff: FeedForward\n",
            "trf_blocks.18.ff.layers: Sequential\n",
            "trf_blocks.18.ff.layers.0: Linear\n",
            "trf_blocks.18.ff.layers.1: GELU\n",
            "trf_blocks.18.ff.layers.2: Linear\n",
            "trf_blocks.18.norm1: LayerNorm\n",
            "trf_blocks.18.norm2: LayerNorm\n",
            "trf_blocks.18.drop_shortcut: Dropout\n",
            "trf_blocks.19: TransformerBlock\n",
            "trf_blocks.19.att: MultiHeadAttention\n",
            "trf_blocks.19.att.W_query: Linear\n",
            "trf_blocks.19.att.W_key: Linear\n",
            "trf_blocks.19.att.W_value: Linear\n",
            "trf_blocks.19.att.out_proj: Linear\n",
            "trf_blocks.19.att.dropout: Dropout\n",
            "trf_blocks.19.ff: FeedForward\n",
            "trf_blocks.19.ff.layers: Sequential\n",
            "trf_blocks.19.ff.layers.0: Linear\n",
            "trf_blocks.19.ff.layers.1: GELU\n",
            "trf_blocks.19.ff.layers.2: Linear\n",
            "trf_blocks.19.norm1: LayerNorm\n",
            "trf_blocks.19.norm2: LayerNorm\n",
            "trf_blocks.19.drop_shortcut: Dropout\n",
            "trf_blocks.20: TransformerBlock\n",
            "trf_blocks.20.att: MultiHeadAttention\n",
            "trf_blocks.20.att.W_query: Linear\n",
            "trf_blocks.20.att.W_key: Linear\n",
            "trf_blocks.20.att.W_value: Linear\n",
            "trf_blocks.20.att.out_proj: Linear\n",
            "trf_blocks.20.att.dropout: Dropout\n",
            "trf_blocks.20.ff: FeedForward\n",
            "trf_blocks.20.ff.layers: Sequential\n",
            "trf_blocks.20.ff.layers.0: Linear\n",
            "trf_blocks.20.ff.layers.1: GELU\n",
            "trf_blocks.20.ff.layers.2: Linear\n",
            "trf_blocks.20.norm1: LayerNorm\n",
            "trf_blocks.20.norm2: LayerNorm\n",
            "trf_blocks.20.drop_shortcut: Dropout\n",
            "trf_blocks.21: TransformerBlock\n",
            "trf_blocks.21.att: MultiHeadAttention\n",
            "trf_blocks.21.att.W_query: Linear\n",
            "trf_blocks.21.att.W_key: Linear\n",
            "trf_blocks.21.att.W_value: Linear\n",
            "trf_blocks.21.att.out_proj: Linear\n",
            "trf_blocks.21.att.dropout: Dropout\n",
            "trf_blocks.21.ff: FeedForward\n",
            "trf_blocks.21.ff.layers: Sequential\n",
            "trf_blocks.21.ff.layers.0: Linear\n",
            "trf_blocks.21.ff.layers.1: GELU\n",
            "trf_blocks.21.ff.layers.2: Linear\n",
            "trf_blocks.21.norm1: LayerNorm\n",
            "trf_blocks.21.norm2: LayerNorm\n",
            "trf_blocks.21.drop_shortcut: Dropout\n",
            "trf_blocks.22: TransformerBlock\n",
            "trf_blocks.22.att: MultiHeadAttention\n",
            "trf_blocks.22.att.W_query: Linear\n",
            "trf_blocks.22.att.W_key: Linear\n",
            "trf_blocks.22.att.W_value: Linear\n",
            "trf_blocks.22.att.out_proj: Linear\n",
            "trf_blocks.22.att.dropout: Dropout\n",
            "trf_blocks.22.ff: FeedForward\n",
            "trf_blocks.22.ff.layers: Sequential\n",
            "trf_blocks.22.ff.layers.0: Linear\n",
            "trf_blocks.22.ff.layers.1: GELU\n",
            "trf_blocks.22.ff.layers.2: Linear\n",
            "trf_blocks.22.norm1: LayerNorm\n",
            "trf_blocks.22.norm2: LayerNorm\n",
            "trf_blocks.22.drop_shortcut: Dropout\n",
            "trf_blocks.23: TransformerBlock\n",
            "trf_blocks.23.att: MultiHeadAttention\n",
            "trf_blocks.23.att.W_query: Linear\n",
            "trf_blocks.23.att.W_key: Linear\n",
            "trf_blocks.23.att.W_value: Linear\n",
            "trf_blocks.23.att.out_proj: Linear\n",
            "trf_blocks.23.att.dropout: Dropout\n",
            "trf_blocks.23.ff: FeedForward\n",
            "trf_blocks.23.ff.layers: Sequential\n",
            "trf_blocks.23.ff.layers.0: Linear\n",
            "trf_blocks.23.ff.layers.1: GELU\n",
            "trf_blocks.23.ff.layers.2: Linear\n",
            "trf_blocks.23.norm1: LayerNorm\n",
            "trf_blocks.23.norm2: LayerNorm\n",
            "trf_blocks.23.drop_shortcut: Dropout\n",
            "trf_blocks.24: TransformerBlock\n",
            "trf_blocks.24.att: MultiHeadAttention\n",
            "trf_blocks.24.att.W_query: Linear\n",
            "trf_blocks.24.att.W_key: Linear\n",
            "trf_blocks.24.att.W_value: Linear\n",
            "trf_blocks.24.att.out_proj: Linear\n",
            "trf_blocks.24.att.dropout: Dropout\n",
            "trf_blocks.24.ff: FeedForward\n",
            "trf_blocks.24.ff.layers: Sequential\n",
            "trf_blocks.24.ff.layers.0: Linear\n",
            "trf_blocks.24.ff.layers.1: GELU\n",
            "trf_blocks.24.ff.layers.2: Linear\n",
            "trf_blocks.24.norm1: LayerNorm\n",
            "trf_blocks.24.norm2: LayerNorm\n",
            "trf_blocks.24.drop_shortcut: Dropout\n",
            "trf_blocks.25: TransformerBlock\n",
            "trf_blocks.25.att: MultiHeadAttention\n",
            "trf_blocks.25.att.W_query: Linear\n",
            "trf_blocks.25.att.W_key: Linear\n",
            "trf_blocks.25.att.W_value: Linear\n",
            "trf_blocks.25.att.out_proj: Linear\n",
            "trf_blocks.25.att.dropout: Dropout\n",
            "trf_blocks.25.ff: FeedForward\n",
            "trf_blocks.25.ff.layers: Sequential\n",
            "trf_blocks.25.ff.layers.0: Linear\n",
            "trf_blocks.25.ff.layers.1: GELU\n",
            "trf_blocks.25.ff.layers.2: Linear\n",
            "trf_blocks.25.norm1: LayerNorm\n",
            "trf_blocks.25.norm2: LayerNorm\n",
            "trf_blocks.25.drop_shortcut: Dropout\n",
            "trf_blocks.26: TransformerBlock\n",
            "trf_blocks.26.att: MultiHeadAttention\n",
            "trf_blocks.26.att.W_query: Linear\n",
            "trf_blocks.26.att.W_key: Linear\n",
            "trf_blocks.26.att.W_value: Linear\n",
            "trf_blocks.26.att.out_proj: Linear\n",
            "trf_blocks.26.att.dropout: Dropout\n",
            "trf_blocks.26.ff: FeedForward\n",
            "trf_blocks.26.ff.layers: Sequential\n",
            "trf_blocks.26.ff.layers.0: Linear\n",
            "trf_blocks.26.ff.layers.1: GELU\n",
            "trf_blocks.26.ff.layers.2: Linear\n",
            "trf_blocks.26.norm1: LayerNorm\n",
            "trf_blocks.26.norm2: LayerNorm\n",
            "trf_blocks.26.drop_shortcut: Dropout\n",
            "trf_blocks.27: TransformerBlock\n",
            "trf_blocks.27.att: MultiHeadAttention\n",
            "trf_blocks.27.att.W_query: Linear\n",
            "trf_blocks.27.att.W_key: Linear\n",
            "trf_blocks.27.att.W_value: Linear\n",
            "trf_blocks.27.att.out_proj: Linear\n",
            "trf_blocks.27.att.dropout: Dropout\n",
            "trf_blocks.27.ff: FeedForward\n",
            "trf_blocks.27.ff.layers: Sequential\n",
            "trf_blocks.27.ff.layers.0: Linear\n",
            "trf_blocks.27.ff.layers.1: GELU\n",
            "trf_blocks.27.ff.layers.2: Linear\n",
            "trf_blocks.27.norm1: LayerNorm\n",
            "trf_blocks.27.norm2: LayerNorm\n",
            "trf_blocks.27.drop_shortcut: Dropout\n",
            "trf_blocks.28: TransformerBlock\n",
            "trf_blocks.28.att: MultiHeadAttention\n",
            "trf_blocks.28.att.W_query: Linear\n",
            "trf_blocks.28.att.W_key: Linear\n",
            "trf_blocks.28.att.W_value: Linear\n",
            "trf_blocks.28.att.out_proj: Linear\n",
            "trf_blocks.28.att.dropout: Dropout\n",
            "trf_blocks.28.ff: FeedForward\n",
            "trf_blocks.28.ff.layers: Sequential\n",
            "trf_blocks.28.ff.layers.0: Linear\n",
            "trf_blocks.28.ff.layers.1: GELU\n",
            "trf_blocks.28.ff.layers.2: Linear\n",
            "trf_blocks.28.norm1: LayerNorm\n",
            "trf_blocks.28.norm2: LayerNorm\n",
            "trf_blocks.28.drop_shortcut: Dropout\n",
            "trf_blocks.29: TransformerBlock\n",
            "trf_blocks.29.att: MultiHeadAttention\n",
            "trf_blocks.29.att.W_query: Linear\n",
            "trf_blocks.29.att.W_key: Linear\n",
            "trf_blocks.29.att.W_value: Linear\n",
            "trf_blocks.29.att.out_proj: Linear\n",
            "trf_blocks.29.att.dropout: Dropout\n",
            "trf_blocks.29.ff: FeedForward\n",
            "trf_blocks.29.ff.layers: Sequential\n",
            "trf_blocks.29.ff.layers.0: Linear\n",
            "trf_blocks.29.ff.layers.1: GELU\n",
            "trf_blocks.29.ff.layers.2: Linear\n",
            "trf_blocks.29.norm1: LayerNorm\n",
            "trf_blocks.29.norm2: LayerNorm\n",
            "trf_blocks.29.drop_shortcut: Dropout\n",
            "trf_blocks.30: TransformerBlock\n",
            "trf_blocks.30.att: MultiHeadAttention\n",
            "trf_blocks.30.att.W_query: Linear\n",
            "trf_blocks.30.att.W_key: Linear\n",
            "trf_blocks.30.att.W_value: Linear\n",
            "trf_blocks.30.att.out_proj: Linear\n",
            "trf_blocks.30.att.dropout: Dropout\n",
            "trf_blocks.30.ff: FeedForward\n",
            "trf_blocks.30.ff.layers: Sequential\n",
            "trf_blocks.30.ff.layers.0: Linear\n",
            "trf_blocks.30.ff.layers.1: GELU\n",
            "trf_blocks.30.ff.layers.2: Linear\n",
            "trf_blocks.30.norm1: LayerNorm\n",
            "trf_blocks.30.norm2: LayerNorm\n",
            "trf_blocks.30.drop_shortcut: Dropout\n",
            "trf_blocks.31: TransformerBlock\n",
            "trf_blocks.31.att: MultiHeadAttention\n",
            "trf_blocks.31.att.W_query: Linear\n",
            "trf_blocks.31.att.W_key: Linear\n",
            "trf_blocks.31.att.W_value: Linear\n",
            "trf_blocks.31.att.out_proj: Linear\n",
            "trf_blocks.31.att.dropout: Dropout\n",
            "trf_blocks.31.ff: FeedForward\n",
            "trf_blocks.31.ff.layers: Sequential\n",
            "trf_blocks.31.ff.layers.0: Linear\n",
            "trf_blocks.31.ff.layers.1: GELU\n",
            "trf_blocks.31.ff.layers.2: Linear\n",
            "trf_blocks.31.norm1: LayerNorm\n",
            "trf_blocks.31.norm2: LayerNorm\n",
            "trf_blocks.31.drop_shortcut: Dropout\n",
            "trf_blocks.32: TransformerBlock\n",
            "trf_blocks.32.att: MultiHeadAttention\n",
            "trf_blocks.32.att.W_query: Linear\n",
            "trf_blocks.32.att.W_key: Linear\n",
            "trf_blocks.32.att.W_value: Linear\n",
            "trf_blocks.32.att.out_proj: Linear\n",
            "trf_blocks.32.att.dropout: Dropout\n",
            "trf_blocks.32.ff: FeedForward\n",
            "trf_blocks.32.ff.layers: Sequential\n",
            "trf_blocks.32.ff.layers.0: Linear\n",
            "trf_blocks.32.ff.layers.1: GELU\n",
            "trf_blocks.32.ff.layers.2: Linear\n",
            "trf_blocks.32.norm1: LayerNorm\n",
            "trf_blocks.32.norm2: LayerNorm\n",
            "trf_blocks.32.drop_shortcut: Dropout\n",
            "trf_blocks.33: TransformerBlock\n",
            "trf_blocks.33.att: MultiHeadAttention\n",
            "trf_blocks.33.att.W_query: Linear\n",
            "trf_blocks.33.att.W_key: Linear\n",
            "trf_blocks.33.att.W_value: Linear\n",
            "trf_blocks.33.att.out_proj: Linear\n",
            "trf_blocks.33.att.dropout: Dropout\n",
            "trf_blocks.33.ff: FeedForward\n",
            "trf_blocks.33.ff.layers: Sequential\n",
            "trf_blocks.33.ff.layers.0: Linear\n",
            "trf_blocks.33.ff.layers.1: GELU\n",
            "trf_blocks.33.ff.layers.2: Linear\n",
            "trf_blocks.33.norm1: LayerNorm\n",
            "trf_blocks.33.norm2: LayerNorm\n",
            "trf_blocks.33.drop_shortcut: Dropout\n",
            "trf_blocks.34: TransformerBlock\n",
            "trf_blocks.34.att: MultiHeadAttention\n",
            "trf_blocks.34.att.W_query: Linear\n",
            "trf_blocks.34.att.W_key: Linear\n",
            "trf_blocks.34.att.W_value: Linear\n",
            "trf_blocks.34.att.out_proj: Linear\n",
            "trf_blocks.34.att.dropout: Dropout\n",
            "trf_blocks.34.ff: FeedForward\n",
            "trf_blocks.34.ff.layers: Sequential\n",
            "trf_blocks.34.ff.layers.0: Linear\n",
            "trf_blocks.34.ff.layers.1: GELU\n",
            "trf_blocks.34.ff.layers.2: Linear\n",
            "trf_blocks.34.norm1: LayerNorm\n",
            "trf_blocks.34.norm2: LayerNorm\n",
            "trf_blocks.34.drop_shortcut: Dropout\n",
            "trf_blocks.35: TransformerBlock\n",
            "trf_blocks.35.att: MultiHeadAttention\n",
            "trf_blocks.35.att.W_query: Linear\n",
            "trf_blocks.35.att.W_key: Linear\n",
            "trf_blocks.35.att.W_value: Linear\n",
            "trf_blocks.35.att.out_proj: Linear\n",
            "trf_blocks.35.att.dropout: Dropout\n",
            "trf_blocks.35.ff: FeedForward\n",
            "trf_blocks.35.ff.layers: Sequential\n",
            "trf_blocks.35.ff.layers.0: Linear\n",
            "trf_blocks.35.ff.layers.1: GELU\n",
            "trf_blocks.35.ff.layers.2: Linear\n",
            "trf_blocks.35.norm1: LayerNorm\n",
            "trf_blocks.35.norm2: LayerNorm\n",
            "trf_blocks.35.drop_shortcut: Dropout\n",
            "final_norm: LayerNorm\n",
            "out_head: Linear\n"
          ]
        }
      ],
      "source": [
        "for name, module in model.named_modules():\n",
        "    print(f\"{name}: {module.__class__.__name__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "c3N58b8RaHEi"
      },
      "outputs": [],
      "source": [
        "def replace_linear_with_lora(model, rank, alpha):\n",
        "    for name, module in model.named_children():\n",
        "        if isinstance(module, torch.nn.Linear) and any(x in name.lower() for x in [\"q\", \"k\", \"v\", \"proj\", \"fc\"]):\n",
        "            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
        "        else:\n",
        "            replace_linear_with_lora(module, rank, alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAYM7IXEaHEi",
        "outputId": "96c43cc6-c6ea-4313-ab67-361b5f81da2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total trainable parameters before: 838,359,040\n",
            "Total trainable parameters after: 0\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters before: {total_params:,}\")\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters after: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra4UTEZjaHEj",
        "outputId": "8e6c620f-0e65-476b-e234-557841f3fdfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total trainable LoRA parameters: 5,898,240\n"
          ]
        }
      ],
      "source": [
        "replace_linear_with_lora(model, rank=16, alpha=16)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable LoRA parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agLJxRgnaHEj",
        "outputId": "deadb1b6-615f-49f0-d9f1-bab211c41bc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPTModel(\n",
            "  (tok_emb): Embedding(50257, 1280)\n",
            "  (pos_emb): Embedding(1024, 1280)\n",
            "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
            "  (trf_blocks): Sequential(\n",
            "    (0): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (1): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (2): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (3): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (4): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (5): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (6): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (7): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (8): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (9): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (10): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (11): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (12): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (13): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (14): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (15): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (16): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (17): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (18): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (19): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (20): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (21): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (22): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (23): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (24): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (25): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (26): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (27): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (28): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (29): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (30): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (31): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (32): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (33): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (34): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (35): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (out_proj): LinearWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_norm): LayerNorm()\n",
            "  (out_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxBG3JXf99xs",
        "outputId": "a604307d-ea0b-4e04-f89e-db82ca39fe7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable: trf_blocks.0.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.0.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.0.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.0.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.0.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.0.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.0.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.0.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.1.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.1.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.1.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.1.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.1.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.1.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.1.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.1.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.2.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.2.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.2.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.2.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.2.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.2.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.2.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.2.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.3.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.3.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.3.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.3.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.3.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.3.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.3.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.3.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.4.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.4.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.4.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.4.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.4.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.4.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.4.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.4.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.5.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.5.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.5.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.5.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.5.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.5.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.5.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.5.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.6.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.6.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.6.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.6.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.6.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.6.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.6.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.6.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.7.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.7.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.7.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.7.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.7.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.7.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.7.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.7.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.8.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.8.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.8.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.8.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.8.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.8.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.8.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.8.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.9.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.9.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.9.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.9.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.9.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.9.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.9.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.9.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.10.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.10.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.10.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.10.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.10.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.10.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.10.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.10.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.11.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.11.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.11.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.11.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.11.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.11.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.11.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.11.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.12.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.12.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.12.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.12.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.12.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.12.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.12.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.12.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.13.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.13.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.13.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.13.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.13.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.13.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.13.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.13.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.14.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.14.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.14.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.14.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.14.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.14.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.14.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.14.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.15.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.15.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.15.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.15.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.15.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.15.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.15.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.15.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.16.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.16.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.16.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.16.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.16.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.16.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.16.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.16.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.17.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.17.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.17.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.17.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.17.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.17.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.17.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.17.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.18.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.18.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.18.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.18.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.18.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.18.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.18.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.18.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.19.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.19.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.19.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.19.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.19.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.19.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.19.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.19.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.20.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.20.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.20.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.20.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.20.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.20.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.20.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.20.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.21.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.21.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.21.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.21.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.21.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.21.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.21.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.21.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.22.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.22.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.22.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.22.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.22.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.22.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.22.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.22.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.23.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.23.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.23.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.23.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.23.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.23.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.23.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.23.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.24.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.24.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.24.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.24.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.24.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.24.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.24.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.24.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.25.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.25.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.25.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.25.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.25.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.25.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.25.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.25.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.26.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.26.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.26.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.26.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.26.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.26.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.26.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.26.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.27.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.27.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.27.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.27.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.27.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.27.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.27.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.27.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.28.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.28.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.28.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.28.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.28.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.28.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.28.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.28.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.29.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.29.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.29.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.29.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.29.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.29.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.29.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.29.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.30.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.30.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.30.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.30.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.30.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.30.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.30.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.30.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.31.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.31.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.31.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.31.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.31.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.31.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.31.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.31.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.32.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.32.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.32.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.32.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.32.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.32.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.32.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.32.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.33.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.33.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.33.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.33.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.33.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.33.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.33.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.33.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.34.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.34.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.34.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.34.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.34.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.34.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.34.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.34.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.35.att.W_query.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.35.att.W_query.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.35.att.W_key.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.35.att.W_key.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.35.att.W_value.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.35.att.W_value.lora.B --> shape: torch.Size([16, 1280])\n",
            "Trainable: trf_blocks.35.att.out_proj.lora.A --> shape: torch.Size([1280, 16])\n",
            "Trainable: trf_blocks.35.att.out_proj.lora.B --> shape: torch.Size([16, 1280])\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(f\"Trainable: {name} --> shape: {param.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6xbsA5vaHEk",
        "outputId": "9e86d83c-e5b3-47bf-85c1-dd29a1db7164"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['story_name', 'average_rating', 'tags', 'body',\n",
            "       'estimated_reading_time', 'publish_date', 'categories'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(\"creepypastas.xlsx\")\n",
        "\n",
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z0we8UMaHEk",
        "outputId": "311723b2-195b-426e-d5e2-ecc5b5ef2f83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3510"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "JBm2RzO1aHEk"
      },
      "outputs": [],
      "source": [
        "df_filtered = df[df['body'].str.len() <= 4000]\n",
        "df_filtered = df_filtered[df_filtered['body'].str.len() > 20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NT-1WwWWaHEk",
        "outputId": "663eae3d-ce6c-4905-f1a3-8f55eea608b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "723"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df_filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "DTtQUEPAj9wM"
      },
      "outputs": [],
      "source": [
        "text_data = df_filtered[\"body\"].dropna().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPtUvI8KkDLA"
      },
      "outputs": [],
      "source": [
        "with open(\"creepypastas.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for story in text_data:\n",
        "        clean_story = story.replace(\"\\n\", \" \").strip()\n",
        "        f.write(clean_story + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "A2MgxF8bcKFy",
        "outputId": "22c8a6eb-fba8-46b6-dc80-90d506619485"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.2.0)\n",
            "Requirement already satisfied: requests>=2.32.2 in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.29.2)\n",
            "Requirement already satisfied: packaging in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: colorama in c:\\users\\david sm\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.66.3->datasets) (0.4.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\david sm\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\david sm\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -orch (c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (c:\\users\\david sm\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3c5024701a394e14b303098a83a032b1",
            "44d5e8a44f374c4695ab3dab6f6b8762",
            "96a7feb501144f3fa87ce6cd8d35392d",
            "5476281a2785430685a79bd779171261",
            "391615cea97246e6a1ef33b5bfb8a03a",
            "8bf885e6376a471c87cc67c65dbc8daf",
            "b2671f32abef40c88f92958c2fd3b2a0",
            "a7f6731ba61c4da097c649e2b476087f",
            "eaf291aa97234bbeaa13a26246876a8b",
            "ed5b404ed5e0449fb44aea739f6ef19f",
            "7dc49080d7b949408fdce9768e078656"
          ]
        },
        "id": "J-9XRxKxaHEk",
        "outputId": "d6685858-0fe9-4ef1-9d09-339f5641ee3e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating train split: 723 examples [00:00, 32920.25 examples/s]\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"text\", data_files={\"train\": \"creepypastas.txt\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iq6t1LW4db1I",
        "outputId": "fab4ad10-2781-4683-e781-87ccaa0ed3b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 723\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f52c0422277a4340b2fb18f1fa5b5f70",
            "4aaab19804604d29945be46468396fb7",
            "5fe8dde7647d4309b94ec7480cf0c600",
            "241dd8f4004b4a8090e75fb1d37542b6",
            "bb2ea5270a924fe188554a3ffc66e6da",
            "63803bee233c4174a56829beb9d47da9",
            "30e175a666fe4f148a57aaca53a3ff72",
            "eb80774e55c848a0b3716942a0ac7dc5",
            "025b85be9f0c4c87978af47df6ffe907",
            "5ba8ac187fa548538cff3bf1e4d4fe29",
            "dc974d881c534c3ea72904ebe81cf1ad"
          ]
        },
        "id": "JvRVt3YtaHEl",
        "outputId": "fb6d9b56-d2bf-48f2-cdbe-e76ce9f3adc7"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(example):\n",
        "    encoding = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=cfg[\"context_length\"] + 1\n",
        "    )\n",
        "\n",
        "    input_ids = encoding[\"input_ids\"]\n",
        "    if len(input_ids) < 2:\n",
        "        return {}\n",
        "\n",
        "    encoding[\"input_ids\"] = input_ids[:-1]\n",
        "    encoding[\"labels\"] = input_ids[1:]\n",
        "\n",
        "    if \"attention_mask\" in encoding:\n",
        "        encoding[\"attention_mask\"] = encoding[\"attention_mask\"][:-1]\n",
        "\n",
        "    return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|| 723/723 [00:02<00:00, 272.05 examples/s]\n"
          ]
        }
      ],
      "source": [
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "nUhoguZSmRxp"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxfwfqG-mbvt",
        "outputId": "96160dc4-29b7-4875-d93f-e5258ce3de01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 723\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SB-BW2NwmvnD",
        "outputId": "7893a4f9-2fd0-4e4d-ea49-7e54884050db"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\DAVID SM\\AppData\\Local\\Temp\\ipykernel_6108\\3579592692.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "from transformers import default_data_collator\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-lora-creepy\",\n",
        "    per_device_train_batch_size=1,\n",
        "    num_train_epochs=1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-4,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=default_data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "PFPa-qUBwxNS"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MWDhipk4mx7p",
        "outputId": "0e6a4b70-7de8-4433-edcb-d1f792ab690d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='361' max='361' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [361/361 2:10:13, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>4.333400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.572800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.009500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.603800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.240400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.617000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.269700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.530300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.526100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.400700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.105000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.370300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.442400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.420600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.682000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.352400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.486500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.077500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.194200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.611500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>1.342900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.423200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.187500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.313100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.443500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>1.346300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>1.416000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.397400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>1.296200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.445400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>1.198700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>1.672100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>1.564500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>1.293500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.267700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.307900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=361, training_loss=1.495055221124369, metrics={'train_runtime': 7836.9827, 'train_samples_per_second': 0.092, 'train_steps_per_second': 0.046, 'total_flos': 0.0, 'train_loss': 1.495055221124369, 'epoch': 0.9986168741355463})"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "_2uWbuIhpfZv"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"modelo_lora.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(\"modelo_lora.pth\", map_location=\"cpu\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "qFaADazDImr5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I was walking in the park when I saw a man in his seventies stand up. He had been running for years now and today it started raining; he wasnt even moving, just standing still on the sidewalk with his legs crossed to the side, as if that is what made\n"
          ]
        }
      ],
      "source": [
        "prompt = \"I was walking in the park when I saw\"\n",
        "output = generate_text(model, tokenizer, prompt, repetition_penalty=1.2)\n",
        "print(output)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
